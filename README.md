# Arquitetura do Pipeline MeteorolÃ³gico (Airflow + Docker + Python + Postgres + Power BI)


## 1ï¸âƒ£ Objetivo

Este projeto implementa um **pipeline de dados meteorolÃ³gicos** desenvolvido em **Python e Apache Airflow**, com integraÃ§Ã£o Ã  **API Open-Meteo** para coleta de informaÃ§Ãµes de **temperatura e precipitaÃ§Ã£o** em capitais brasileiras.

O pipeline realiza:
- ğŸ”„ **OrquestraÃ§Ã£o automÃ¡tica** das tarefas via Airflow  
- â˜ï¸ **Coleta de dados** da API pÃºblica **Open-Meteo** (temperatura, precipitaÃ§Ã£o, etc.)  
- ğŸ§® **TransformaÃ§Ãµes e validaÃ§Ãµes** com Python  
- ğŸ—„ï¸ **Armazenamento** dos resultados em banco de dados PostgreSQL  
- ğŸ“Š **IntegraÃ§Ã£o com Power BI** para visualizaÃ§Ã£o dos dados

# ğŸŒ¤ï¸ Pipeline MeteorolÃ³gico (Airflow + Docker + Python)

Este repositÃ³rio demonstra a construÃ§Ã£o de um pipeline de dados meteorolÃ³gicos usando **Apache Airflow** dentro de um ambiente **containerizado com Docker**.

---

## ğŸš€ Tecnologias
- Python  
- Apache Airflow  
- Docker & Docker Compose  
- PostgreSQL  
- Open-Meteo API  
- Power BI (consumo dos dados)

---

## ğŸ“ Estrutura do projeto

```bash
api_clima/
â”œâ”€â”€ docker-compose/
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ img/
â”‚   â”œâ”€â”€ projeto.png
â”‚   â””â”€â”€ dag.png
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ meteo_pipeline.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```


O objetivo Ã© demonstrar, de forma prÃ¡tica, como construir uma DAG completa â€” desde a **extraÃ§Ã£o de dados brutos via API**, atÃ© a **carga estruturada em banco de dados relacional**, dentro de um ambiente **containerizado e reproduzÃ­vel**.

## 2ï¸âƒ£ Arquitetura

A arquitetura reflete um fluxo real de Engenharia de Dados, com ingestÃ£o incremental, controle de histÃ³rico e modularizaÃ§Ã£o das etapas (ingestÃ£o, processamento e persistÃªncia).

![Arquitetura](./img/projeto.png)


## 3ï¸âƒ£ DAGs e Tasks

A DAG principal (`meteo_historico_nivel2`) realiza o fluxo ETL completo:

![DAG](./img/dag.png)

### Principais Tasks
- `extract_openmeteo`: coleta dados da API
- `transform_data`: normaliza e aplica validaÃ§Ãµes bÃ¡sicas
- `load_to_postgres`: insere no banco de dados
- `check_quality`: valida a integridade e formato dos dados


## 4ï¸âƒ£ Observabilidade

- Monitoramento de execuÃ§Ã£o via **Airflow UI**
- Logs estruturados por task com timestamp, volume e status
- PolÃ­tica de **retries automÃ¡ticos** e alertas de falha (configurÃ¡vel)

## 5ï¸âƒ£ GovernanÃ§a e Data Quality
Como o pipeline garante integridade, schema e histÃ³rico.

## 6ï¸âƒ£ Infraestrutura
ServiÃ§os do Docker Compose e volumes.

## 7ï¸âƒ£ ExecuÃ§Ã£o e Agendamento
Como rodar manualmente e como o cron diÃ¡rio foi configurado.


### â–¶ï¸ ExecuÃ§Ã£o manual da DAG (Ãºltimos 6 meses)

Para executar manualmente a DAG `meteo_historico_nivel2` e coletar dados dos Ãºltimos seis meses, utilize o comando abaixo no terminal:

```bash
docker exec -it airflow-airflow-scheduler-1 bash -lc \
  "airflow dags trigger meteo_historico_nivel2 \
   --conf '{\"start\":\"2025-04-23\",\"end\":\"2025-10-23\"}'"
```

   
ğŸ’¡ ExplicaÃ§Ã£o rÃ¡pida:

- docker exec -it airflow-airflow-scheduler-1 bash -lc â†’ acessa o container do scheduler do Airflow.
- airflow dags trigger meteo_historico_nivel2 â†’ dispara manualmente a DAG.
- --conf â†’ passa as datas de inÃ­cio e fim do intervalo para o script dentro da DAG.

As datas aqui cobrem os Ãºltimos seis meses (23/04/2025 â†’ 23/10/2025).

## 8ï¸âƒ£ PrÃ³ximos Passos
Melhorias futuras (Data Lake, alertas, dashboard).



